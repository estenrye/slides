- hosts: 127.0.0.1
  connection: local
  vars_files:
    - group_vars/all.yml
  tasks:
    - name: Add chart repos
      community.kubernetes.helm_repository:
        name: "{{ item.name }}"
        repo_url: "{{ item.repo_url }}"
      loop:
        - name: prometheus-community
          repo_url: https://prometheus-community.github.io/helm-charts
        - name: ingress-nginx
          repo_url: https://kubernetes.github.io/ingress-nginx
        - name: jetstack
          repo_url: https://charts.jetstack.io
        - name: kubernetes-dashboard
          repo_url: https://kubernetes.github.io/dashboard/
        - name: tremolo
          repo_url: https://nexus.tremolo.io/repository/helm/
        # - name: linkerd-stable
        #   repo_url: https://helm.linkerd.io/stable

    # Deploying a ingress controller is the first chart we deploy
    # on the cluster.  All services will be exposed through the
    # ingress controller.
    - name: Deploy the NGINX Ingress Controller chart
      community.kubernetes.helm:
        name: ingress-nginx
        chart_ref: ingress-nginx/ingress-nginx
        chart_version: "{{ helm_chart_version_ingress_nginx }}"
        release_namespace: ingress
        create_namespace: true
        values_files:
          - files/helm_values/ingress-nginx.yml

    # The acme cert-manager issuers need an issuer key to request
    # certificates using an acme account.  These tasks generate the
    # secret content for those issuers.
    - name: Generate an OpenSSL private key with the default values (4096 bits, RSA)
      community.crypto.openssl_privatekey:
        path: files/secrets/acme_issuer_key.pem
        state: present
        return_content: true
      register: acme_issuer_key
    - name: generate value file for acmeCloudflareIssuer Helm Chart
      ansible.builtin.template:
        src: templates/acme_issuer_key.yml.j2
        dest: files/secrets/acme_issuer_key.yml

    # This chart installs the cert-manager operator.
    # This operator watches the kubernetes api for Ingress objects
    # with annotations like this one:
    #     - cert-manager.io/issuer: {{ .Values.certManagerIssuer }}
    # if a matching Issuer exists, cert-manager will manage and
    # automatically rotate the certificate.
    - name: Deploy cert-manager chart inside cert-manager namespace (and create it)
      community.kubernetes.helm:
        name: cert-manager
        chart_ref: jetstack/cert-manager
        chart_version: "{{ helm_chart_version_cert_manager }}"
        release_namespace: cert-manager
        create_namespace: true
        values_files:
          - files/helm_values/cert-manager.yml

    # The next set of charts we deploy are the cloudflare acme
    # certificate issuers.  Currently these are deployed one
    # per namespace to at some point take advantage of the capability
    # to scope certificates generated to specific zones based on
    # the namespace name.  For example:
    #    - monitoring.dev.ryezone.com for the monitoriong database.
    # This could allow us to control allowed dns zones and give teams
    # some autonomy to define their dns names as they desire.
    # May end up consolidating this at the cluster level, as I have yet
    # to actually use that capability.
    - name: Deploy cert-manager configuration chart inside ingress namespace (and create it)
      community.kubernetes.helm:
        name: prod
        chart_ref: files/helm_charts/acmeCloudflareIssuer
        chart_version: 0.1.0
        release_namespace: "{{ item }}"
        create_namespace: true
        values_files:
          - files/secrets/acme_issuer_key.yml
        values:
          acmeUseProductionServer: true
      loop:
        - ingress
        - monitoring
        - kube-dashboard
        - openunison

    # The next set of charts we deploy deploy the kubernetes dashboard.
    # At this point we are not exposing the dashboard as we will do that
    # as part of the OpenUnison deployment where we secure the kubernetes
    # dashboard and api using Open ID Connect and Okta.
    - name: Deploy kube-dashboard chart in kube-dashboard namespace (and create it)
      community.kubernetes.helm:
        name: "{{ kubernetes_dashboard_release }}"
        chart_ref: kubernetes-dashboard/kubernetes-dashboard
        chart_version: "{{ helm_chart_version_kubernetes_dashboard }}"
        release_namespace: "{{ kubernetes_dashboard_namespace }}"
        create_namespace: true
    
    # TODO: Delete this code, no longer necessary
    # - name: Deploy ingress manifests in kube-dashboard namespace (and create it)
    #   community.kubernetes.helm:
    #     name: ingress
    #     chart_ref: files/helm_charts/kubeDashboardIngress
    #     release_namespace: "{{ kubernetes_dashboard_namespace }}"
    #     create_namespace: true
    #     values:
    #       zone: "{{ kubernetes_zone }}"

    # Now that the dashboard is deployed, we want to expose it and
    # enable our Okta authenticated users to authenticate to it via
    # both the web UI and kubectl.  To do this, we need an Open ID
    # connect proxy named OpenUnison.  OpenUnison and Orchestra
    # require a couple of secrets to provide authentication services
    # to the cluster.  Most of these can be automatically generated.
    # one of these secrets will need to be manually created prior to
    # executing this playbook.  Once you have provisioned the Okta
    # OIDC web application, you will need to place your OIDC token in
    # plain text at files/secrets/oidc_client_secret.
    - name: Create OpenUnison secret
      community.kubernetes.k8s:
        state: present
        definition:
          apiVersion: v1
          type: Opaque
          metadata:
            name: orchestra-secrets-source
            namespace: openunison
          data:
            K8S_DB_SECRET: "{{ lookup('password', 'files/secrets/k8s_db_secret chars=ascii_letters,digits length=64') | b64encode }}"
            unisonKeystorePassword: "{{ lookup('password', 'files/secrets/unisonKeystorePassword chars=ascii_letters,digits length=256') | b64encode }}"
            OIDC_CLIENT_SECRET: "{{ lookup('password', 'files/secrets/oidc_client_secret chars=ascii_letters,digits length=8') | b64encode }}"
            grafana_oidc_client_secret: "{{ lookup('password', 'files/secrets/grafana_oidc_client_secret chars=ascii_letters,digits length=256') | b64encode }}"
          kind: Secret

    # Now that we have the secret material placed for Orchestra
    # OpenUnison, we can now install the OpenUnison Operator.
    - name: Deploy OpenUnison Operator chart inside openunison namespace (and create it)
      community.kubernetes.helm:
        name: openunison
        chart_ref: tremolo/openunison-operator
        chart_version: "{{ helm_chart_version_openunison }}"
        release_namespace: openunison
        create_namespace: true

    # Telling OpenUnison to trust authentication redirects
    # from Grafana.  Note, new trusts added after the Orchestra
    # chart is deployed will require deleting the Orchestra pod
    # to enable.
    - name: Create OpenUnison Trust for Grafana
      community.kubernetes.k8s:
        state: present
        src: files/kubernetes_manifests/openunison/{{item}}
      loop:
        - trust_grafana.yml
        - badge_grafana.yml

    # With the operator installed, we can now deploy Orchestra.
    # Orchestra will provide us with a single sign-on portal for
    # cluster resources.  Later we will configure other dashboards
    # to use openunison for authentication.
    - name: Deploy OpenUnison Operator chart inside openunison namespace (and create it)
      community.kubernetes.helm:
        name: orchestra
        chart_ref: tremolo/openunison-k8s-login-oidc
        chart_version: "{{ helm_chart_version_orchestra }}"
        release_namespace: openunison
        create_namespace: true
        values_files:
          - files/helm_values/orchestra.yml

    # now that we have openunison and orchestra installed, we now
    # need to configure the kubernetes api to trust openunison as
    # an OIDC provider.  This involves two parts:
    #  1. Downloading the OpenUnison CA certificate for upload to
    #     the controlplane nodes.
    #  2. Downloading the patch configuration to insert into the
    #     kubernetes api static manifest.
    - name: Get OpenUnison CA Certificate
      community.kubernetes.k8s_info:
        api_version: v1
        kind: Secret
        namespace: openunison
        name: ou-tls-certificate
      register: ou_ca
    - name: Get the api server config for oidc authentication
      community.kubernetes.k8s_info:
        api_version: v1
        kind: ConfigMap
        namespace: openunison
        name: api-server-config
      register: ou_api_config 
    - name: Write openunison ca certificate to localhost
      ansible.builtin.copy:
        content: '{{ ou_ca.resources[0].data["tls.crt"] | b64decode }}'
        dest: files/secrets/ou-ca.pem
    - name: Write openunison api server config to localhost
      ansible.builtin.copy:
        content: "{{ ('- ' + ou_api_config.resources[0].data['oidc-api-server-flags']) | replace('\n', '\n- ') | indent(width=4, first=true)}}"
        dest: files/secrets/ou-config.yml

    # Apply any group role bindings to enable users to authenticate
    # with the appropriate permissions.
    - name: apply group role bindings
      community.kubernetes.k8s:
        state: present
        src: "files/kubernetes_manifests/role_based_access_control/{{ item }}.yml"
      loop:
        - acl_okta_k8s_cluster_admin_dev

# Now we take a brief excursion back out to the controlplane nodes.
# Here we install the openunison CA certificate on each controlplane
# node and patch the kubernetes api configuration in to tell the
# cluster to trust openunison as an OIDC token provider.
- hosts: controlplane
  vars_files:
    - group_vars/all.yml
  tasks:
    - name: copy ou ca to controlplane
      ansible.builtin.copy:
        src: files/secrets/ou-ca.pem
        dest: /etc/kubernetes/pki/ou-ca.pem
      become: true
    - name: add oidc config to kube-apiserver manifest
      ansible.builtin.blockinfile:
        insertafter: "    - kube-apiserver"
        marker: "    # {mark}: ANSIBLE MANAGED BLOCK - OIDC FLAGS"
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        block: "{{ lookup('file', 'files/secrets/ou-config.yml') }}"
      become: true

# Now back to our local host.
- hosts: 127.0.0.1
  connection: local
  vars_files:
    - group_vars/all.yml
  tasks:
    # Now we are going to start installing our monitoring platform.
    # First we need to create a secret for Grafana to access the
    # openunison client secret we created earlier for it to trust
    # openunison as an authentication provider.
    - name: Create OpenUnison OIDC secret for Grafana
      community.kubernetes.k8s:
        state: present
        definition:
          apiVersion: v1
          type: Opaque
          metadata:
            name: grafana-oidc-client-secret
            namespace: monitoring
          data:
            oidc_client_secret: "{{ lookup('password', 'files/secrets/grafana_oidc_client_secret chars=ascii_letters,digits length=256') | b64encode }}"
          kind: Secret

    # Next we deploy the Kube Prometheus Stack chart.
    - name: Deploy Prometheus chart inside monitoring namespace (and create it)
      community.kubernetes.helm:
        name: kube-prometheus
        chart_ref: prometheus-community/kube-prometheus-stack
        chart_version: "{{ helm_chart_version_prometheus }}"
        release_namespace: monitoring
        create_namespace: true
        values_files:
          - files/helm_values/kube_prometheus_stack.yml

    # Ingress rules that expose dashboards in the monitoring namespace.
    - name: Deploy ingress manifests in monitoring namespace (and create it)
      community.kubernetes.helm:
        name: ingress
        chart_ref: files/helm_charts/kubePrometheusStackIngress
        release_namespace: monitoring
        create_namespace: true
        values:
          zone: "{{ kubernetes_zone }}"

    # - name: Configure Cloudflare API Key for cert-manager
    #   community.kubernetes.k8s:
    #     state: present
    #     definition:
    #       apiVersion: v1
    #       kind: Secret
    #       metadata:
    #         name: cloudflare-api-token-secret
    #         namespace: cert-manager
    #       type: Opaque
    #       stringData:
    #         api-token: "{{ cert_manager_cloudflare_api_key }}"

    # - name: Configure LetsEncrypt Staging Issuer for cert-manager
    #   community.kubernetes.k8s:
    #     state: present
    #     definition:
    #       apiVersion: cert-manager.io/v1
    #       kind: ClusterIssuer
    #       metadata:
    #         name: letsencrypt-staging
    #         namespace: cert-manager
    #       spec:
    #         acme:
    #           email: "{{ cert_manager_cloudflare_account_email }}"
    #           server: https://acme-staging-v02.api.letsencrypt.org/directory
    #           preferredChain: "ISRG Root X1"
    #           privateKeySecretRef:
    #             name: letsencrypt-issuer-account-key
    #           solvers:
    #             - dns01:
    #                 cloudflare:
    #                   email: "{{ cert_manager_cloudflare_account_email }}"
    #                   apiKeySecretRef:
    #                     name: cloudflare-apikey-secret
    #                     key: apikey
    #               selector:
    #                 dnsZones:
    #                 - "{{ kubernetes_zone }}"
    #             - http01:
    #                 ingress:
    #                   class: nginx
    #                 selector:
    #                   matchLabels:
    #                     "use-http01-solver": "true"

    # - name: Deploy Linkerd chart inside linkerd namespace (and create it)
    #   community.kubernetes.helm:
    #     name: linkerd
    #     chart_ref: linkerd-stable/linkerd2
    #     chart_version: "{{ helm_chart_version_linkerd }}"
    #     release_namespace: linkerd
    #     create_namespace: true
